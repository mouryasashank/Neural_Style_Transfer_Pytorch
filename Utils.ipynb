{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jg2CYOzLO6-5"
      },
      "outputs": [],
      "source": [
        "# utils\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "%run '/content/vgg_nets.ipynb'\n",
        "\n",
        "\n",
        "\n",
        "IMAGENET_MEAN_255 = [123.675, 116.28, 103.53]\n",
        "IMAGENET_STD_NEUTRAL = [1, 1, 1]\n",
        "\n",
        "\n",
        "#\n",
        "# Image manipulation util functions\n",
        "#\n",
        "\n",
        "def load_image(img_path, target_shape=None):\n",
        "    if not os.path.exists(img_path):\n",
        "        raise Exception(f'Path does not exist: {img_path}')\n",
        "    img = cv.imread(img_path)[:, :, ::-1]  # [:, :, ::-1] converts BGR (opencv format...) into RGB\n",
        "\n",
        "    if target_shape is not None:  # resize section\n",
        "        if isinstance(target_shape, int) and target_shape != -1:  # scalar -> implicitly setting the height\n",
        "            current_height, current_width = img.shape[:2]\n",
        "            new_height = target_shape\n",
        "            new_width = int(current_width * (new_height / current_height))\n",
        "            img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)\n",
        "        else:  # set both dimensions to target shape\n",
        "            img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)\n",
        "\n",
        "    # this need to go after resizing - otherwise cv.resize will push values outside of [0,1] range\n",
        "    img = img.astype(np.float32)  # convert from uint8 to float32\n",
        "    img /= 255.0  # get to [0, 1] range\n",
        "    return img\n",
        "\n",
        "\n",
        "def prepare_img(img_path, target_shape, device):\n",
        "    img = load_image(img_path, target_shape=target_shape)\n",
        "\n",
        "    # normalize using ImageNet's mean\n",
        "    # [0, 255] range worked much better for me than [0, 1] range (even though PyTorch models were trained on latter)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255)),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN_255, std=IMAGENET_STD_NEUTRAL)\n",
        "    ])\n",
        "\n",
        "    img = transform(img).to(device).unsqueeze(0)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_image(img, img_path):\n",
        "    if len(img.shape) == 2:\n",
        "        img = np.stack((img,) * 3, axis=-1)\n",
        "    cv.imwrite(img_path, img[:, :, ::-1])  # [:, :, ::-1] converts rgb into bgr (opencv contraint...)\n",
        "\n",
        "\n",
        "def generate_out_img_name(config):\n",
        "    prefix = os.path.basename(config['content_img_name']).split('.')[0] + '_' + os.path.basename(config['style_img_name']).split('.')[0]\n",
        "    # called from the reconstruction script\n",
        "    if 'reconstruct_script' in config:\n",
        "        suffix = f'_o_{config[\"optimizer\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}{config[\"img_format\"][1]}'\n",
        "    else:\n",
        "        suffix = f'_o_{config[\"optimizer\"]}_i_{config[\"init_method\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}_cw_{config[\"content_weight\"]}_sw_{config[\"style_weight\"]}_tv_{config[\"tv_weight\"]}{config[\"img_format\"][1]}'\n",
        "    return prefix + suffix\n",
        "\n",
        "\n",
        "def save_and_maybe_display(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display=False):\n",
        "    saving_freq = config['saving_freq']\n",
        "    out_img = optimizing_img.squeeze(axis=0).to('cpu').detach().numpy()\n",
        "    out_img = np.moveaxis(out_img, 0, 2)  # swap channel from 1st to 3rd position: ch, _, _ -> _, _, chr\n",
        "\n",
        "    # for saving_freq == -1 save only the final result (otherwise save with frequency saving_freq and save the last pic)\n",
        "    if img_id == num_of_iterations-1 or (saving_freq > 0 and img_id % saving_freq == 0):\n",
        "        img_format = config['img_format']\n",
        "        out_img_name = str(img_id).zfill(img_format[0]) + img_format[1] if saving_freq != -1 else generate_out_img_name(config)\n",
        "        dump_img = np.copy(out_img)\n",
        "        dump_img += np.array(IMAGENET_MEAN_255).reshape((1, 1, 3))\n",
        "        dump_img = np.clip(dump_img, 0, 255).astype('uint8')\n",
        "        cv.imwrite(os.path.join(dump_path, out_img_name), dump_img[:, :, ::-1])\n",
        "\n",
        "    if should_display:\n",
        "        plt.imshow(np.uint8(get_uint8_range(out_img)))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def get_uint8_range(x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x -= np.min(x)\n",
        "        x /= np.max(x)\n",
        "        x *= 255\n",
        "        return x\n",
        "    else:\n",
        "        raise ValueError(f'Expected numpy array got {type(x)}')\n",
        "\n",
        "\n",
        "#\n",
        "# End of image manipulation util functions\n",
        "#\n",
        "\n",
        "\n",
        "# initially it takes some time for PyTorch to download the models into local cache\n",
        "def prepare_model(model, device):\n",
        "    # we are not tuning model weights -> we are only tuning optimizing_img's pixels! (that's why requires_grad=False)\n",
        "    experimental = False\n",
        "    if model == 'vgg16':\n",
        "        if experimental:\n",
        "            # much more flexible for experimenting with different style representations\n",
        "            model = Vgg16Experimental(requires_grad=False, show_progress=True)\n",
        "        else:\n",
        "            model = Vgg16(requires_grad=False, show_progress=True)\n",
        "    elif model == 'vgg19':\n",
        "        model = Vgg19(requires_grad=False, show_progress=True)\n",
        "    else:\n",
        "        raise ValueError(f'{model} not supported.')\n",
        "\n",
        "    content_feature_maps_index = model.content_feature_maps_index\n",
        "    style_feature_maps_indices = model.style_feature_maps_indices\n",
        "    layer_names = model.layer_names\n",
        "\n",
        "    content_fms_index_name = (content_feature_maps_index, layer_names[content_feature_maps_index])\n",
        "    style_fms_indices_names = (style_feature_maps_indices, layer_names)\n",
        "    return model.to(device).eval(), content_fms_index_name, style_fms_indices_names\n",
        "\n",
        "\n",
        "def gram_matrix(x, should_normalize=True):\n",
        "    (b, ch, h, w) = x.size()\n",
        "    features = x.view(b, ch, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t)\n",
        "    if should_normalize:\n",
        "        gram /= ch * h * w\n",
        "    return gram\n",
        "\n",
        "\n",
        "def total_variation(y):\n",
        "    return torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n",
        "           torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1rdlkeVPCda"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}